{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tramento dos dados brutos\r\n",
    "\r\n",
    "Após a carga dos dataset a partir dos arquivos (doCarga), nesta etapa os dados sofrerão limpesa e tratamento, aonde serão retiradas \"sugeiras grosseiras\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import unicodedata\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tratamentos de Colunas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# retira stop words do nome dos usuários reais\r\n",
    "def limpaTexto( entrada, stop_words ):\r\n",
    "    \r\n",
    "    listaRetiraInicio = [] # ['BB','Z ', 'MC ']\r\n",
    "    listaRetiraFim    = [] # [' CC']\r\n",
    "    listaCaracteresIndesejados = [\"\t\", # tabulação\r\n",
    "                                \"*\",\r\n",
    "                                \"#\",\r\n",
    "                                \"|\",\r\n",
    "                                '¥'\r\n",
    "                                ]\r\n",
    "\r\n",
    "    # PROBLEMA: TEM QUE VERIFICAR A LISTA DE STOPWORDS PARA QUE ELA \r\n",
    "    # NÃO RETIRE DESCRIÇÃO VÁLIDA DE PRODUTOS.\r\n",
    "    # retira stopwords\r\n",
    "    entrada = str(entrada)\r\n",
    "    entrada = entrada.split(\" \")\r\n",
    "    \r\n",
    "    saida = \"\"\r\n",
    "    for item in entrada:\r\n",
    "        if item.lower() not in stop_words :\r\n",
    "            saida = saida + item + \" \"\r\n",
    "    \r\n",
    "    # retira acentos\r\n",
    "    aux   = str(saida)\r\n",
    "    nfkd  = unicodedata.normalize('NFKD', aux)\r\n",
    "    saida = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\r\n",
    "    \r\n",
    "    # transforma em maiúsculo, retira espaços dos extremos (TRIM), \r\n",
    "    saida = saida.upper().strip()\r\n",
    "    \r\n",
    "    # retira caracteres indesejados --> cuidado para não unir palavras separadas pelo caracter\r\n",
    "    for item in listaCaracteresIndesejados:\r\n",
    "        saida = saida.replace(item,\"\")    \r\n",
    "    \r\n",
    "    # retira espaços duplos dentro da string\r\n",
    "    saida = saida.replace(\"  \",\" \")\r\n",
    "    while saida.find('  ') != -1:\r\n",
    "        saida = saida.replace(\"  \",\" \")\r\n",
    "              \r\n",
    "    # retira inicios de strings que constam na lista listaRetiraInicio\r\n",
    "    for item in listaRetiraInicio:\r\n",
    "        # retira espaços duplos dentro do item, pois o mesmo já foi feito no texto a pesquisar\r\n",
    "        item = item.replace(\"  \",\" \")\r\n",
    "        while item.find('  ') != -1:\r\n",
    "            item = item.replace(\"  \",\" \")\r\n",
    "            \r\n",
    "        if saida.startswith(item):\r\n",
    "            tamanhoItem  = len(item)\r\n",
    "            tamanhoSaida = len(saida)            \r\n",
    "            if tamanhoSaida > tamanhoItem:\r\n",
    "                saida = saida[tamanhoItem:]\r\n",
    "    \r\n",
    "    # retira fins de strings que constam na lista listaRetiraFim\r\n",
    "    for item in listaRetiraFim:\r\n",
    "        # retira espaços duplos dentro do item, pois o mesmo já foi feito no texto a pesquisar\r\n",
    "        item = item.replace(\"  \",\" \")\r\n",
    "        while item.find('  ') != -1:\r\n",
    "            item = item.replace(\"  \",\" \")        \r\n",
    "        \r\n",
    "        if saida.endswith(item):\r\n",
    "            tamanhoItem  = len(item)\r\n",
    "            tamanhoSaida = len(saida)\r\n",
    "            if tamanhoSaida > tamanhoItem:\r\n",
    "                saida = saida[:tamanhoSaida-tamanhoItem]   \r\n",
    "    \r\n",
    "    # retira espaços duplos dentro da string. Necessário pois pode ter sobrado \r\n",
    "    # espaço depois das alterações\r\n",
    "    saida = saida.strip()\r\n",
    "    \r\n",
    "    return saida"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def doLimpeza(df_original):\r\n",
    "    # Apaga linhas cuja coluna 'NovaDescricao' esteja em branco\r\n",
    "    df_original['NovaDescricao'].dropna( axis=0, inplace=True ) \r\n",
    "\r\n",
    "\r\n",
    "     # Apaga colunas que não serão usadas\r\n",
    "    df_original.drop(['Desc_Catalogo', 'NCM_NFe', 'Desc_Anexo_IV', 'NCM_Calc_Prov', 'Cor_NCM_Calc', 'NCM_Calc', \r\n",
    "        'Item_Anexo_IV', 'Prod_ST', 'Prod_FCP'], axis=1, inplace=True)\r\n",
    "    \r\n",
    "    return df_original"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def doDropDuplicados(df_original):\r\n",
    "    ## Elimina linhas inconsistentes, repetidas. Mantém a primeira delas\r\n",
    "    #antes = df_original.shape[0]\r\n",
    "    df_original.drop_duplicates( subset = [\"NovaDescricao\"], keep='first', inplace=True) \r\n",
    "    #print( 'Eliminados', antes - df_original.shape[0], 'registros')\r\n",
    "    #del antes\r\n",
    "    return df_original "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def doDropIndesejados(df_original):\r\n",
    "    # Retira categorias indesejadas\r\n",
    "    retirar = ['nannãonão','nansimnão','nansimsim']\r\n",
    "    mask = ~df_original['Categoria'].isin(retirar)\r\n",
    "    filtrado = df_original[mask].copy()\r\n",
    "    df_original = filtrado.copy()\r\n",
    "    del filtrado, retirar\r\n",
    "    return df_original"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tratamento de Strings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def doListaPalavras(df):\r\n",
    "    # Gera lista com todas as palavras, com repetições\r\n",
    "    # CountVectorizer retira duplicidade\r\n",
    "    \r\n",
    "    total_linhas = df.shape[0]\r\n",
    "    lista_palavras = []\r\n",
    "    for i in range(0,total_linhas):\r\n",
    "        texto = df.iloc[i, 2].split()\r\n",
    "        tamanho_linha = len(texto)\r\n",
    "        for j in range(0,tamanho_linha):\r\n",
    "            lista_palavras.append( texto[j]) \r\n",
    "            \r\n",
    "    # Gera DF com a contagem de cada palavra\r\n",
    "    lista_palavras = pd.Series(data = lista_palavras)\r\n",
    "    df_lista_palavras = pd.DataFrame(data = lista_palavras.value_counts() )#, index = lista_palavras)\r\n",
    "    df_lista_palavras.reset_index(drop = False, inplace = True)\r\n",
    "    df_lista_palavras.columns = ['Palavra', 'QTD']\r\n",
    "\r\n",
    "    #libera memoria\r\n",
    "    del total_linhas, lista_palavras, texto, tamanho_linha,\r\n",
    "    return df_lista_palavras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def doRemoveChar(Lista):\r\n",
    "    # elimina palavras de 1 caracter\r\n",
    "    mask = (Lista['Palavra'].str.len() == 1)\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Eliminar = []\r\n",
    "    Eliminar = list(df_aux['Palavra'])\r\n",
    "    mask = ~(Lista['Palavra'].str.len() == 1)\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Lista = df_aux.copy()\r\n",
    "\r\n",
    "    del df_aux, mask\r\n",
    "    return Lista, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def doRemoveNumeros(Lista, Eliminar):\r\n",
    "    # Elimina palavras que são somente numeros, exceto os de 13 algarismos ( possíveis EAN)\r\n",
    "\r\n",
    "    # Separa possíveis códigos EAN\r\n",
    "    mask1 = Lista.Palavra.str.len() == 13\r\n",
    "    mask2 = Lista.Palavra.str.isdigit()\r\n",
    "    df_aux = Lista[~(mask1 & mask2)]\r\n",
    "    # elimina números\r\n",
    "    mask2 = df_aux.Palavra.str.isdigit()\r\n",
    "    df_aux = df_aux[mask2]\r\n",
    "\r\n",
    "    Eliminar = Eliminar + list(df_aux['Palavra'])\r\n",
    "    mask = ~Lista['Palavra'].isin( list(df_aux['Palavra']) )\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Lista = df_aux.copy()\r\n",
    "    del df_aux, mask1, mask2\r\n",
    "    return Lista, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tratamento de Arquivos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def doMapOcorrencia(Origem, termo, Caminho):\r\n",
    "    result = False\r\n",
    "    mask1  = Origem['Palavra'].str.endswith(termo)\r\n",
    "    mask2  = Origem['Palavra'].str.len() < 80\r\n",
    "    mask3  = Origem['QTD'] > 10 \r\n",
    "    df_aux = Origem[ mask1 & mask2 & mask3 ]\r\n",
    "\r\n",
    "    if df_aux.shape[0] > 1:\r\n",
    "        df_aux.to_csv(Caminho + \"ETL_\" + termo + \"_a_verificar.csv\", index = True, columns = [\"Palavra\", \"QTD\"], sep = ';', encoding = \"utf-8\")\r\n",
    "        result = True\r\n",
    "    #else:\r\n",
    "        #print( 'Não há palavras terminadas com \"{}\", ou há somente uma.'.format(termo) )\r\n",
    "    del df_aux, mask1, mask2, mask3\r\n",
    "    return result \r\n",
    "\r\n",
    "def doMapQtd(Origem, Caminho):\r\n",
    "    #Trata palavras pela quantidade delas\r\n",
    "    result = False\r\n",
    "    mask  = Origem['QTD'] > 1000 \r\n",
    "    df_aux = Origem[ mask ]\r\n",
    "    if df_aux.shape[0] > 1:\r\n",
    "        df_aux.to_csv(Caminho + \"ETL_QUANTIDADE_a_verificar.csv\", index = True, columns = [\"Palavra\", \"QTD\"], sep = ';', encoding = \"utf-8\")\r\n",
    "        result = True\r\n",
    "    #else:\r\n",
    "        #print( 'Não há palavras mais de 1000 ocorrências' )\r\n",
    "    del df_aux, mask\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def EliminaPalavrasVindasDeArquivo( nome_arquivo, Lista ):\r\n",
    "    df_aux = pd.read_csv(nome_arquivo, index_col = ['Indice'], sep = ';',)\r\n",
    "    Lista.append(list(df_aux['Palavra']))\r\n",
    "    mask = ~Lista['Palavra'].isin( list(df_aux['Palavra']) )\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Lista = df_aux.copy()\r\n",
    "    del df_aux, mask\r\n",
    "    return Lista"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execução"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def doTrataColunas(Origem, stop_words ):\r\n",
    "    ## TRATAMENTO DE COLUNAS ##\r\n",
    "    \r\n",
    "    # Concatena 3 colunas para formar a coluna 'Categoria', que é a classificação do produto\r\n",
    "    Origem['Categoria'] = Origem['Item_Anexo_IV'].astype(str) + Origem['Prod_ST'] + Origem['Prod_FCP']\r\n",
    "    \r\n",
    "    Origem['NovaDescricao'] = Origem.apply(lambda row: limpaTexto(row.Desc_Catalogo, stop_words) , axis=1)\r\n",
    "\r\n",
    "    Origem.rename(columns={'Nome': 'Remetente'}, inplace = True)\r\n",
    " \r\n",
    "    Origem = doLimpeza(Origem)   \r\n",
    "\r\n",
    "    Origem = doDropDuplicados(Origem)\r\n",
    "\r\n",
    "    return Origem"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def doTrataTextos(Origem):\r\n",
    "    ## TRATAMENTO DE TEXTOS ##\r\n",
    "    Palavras = doListaPalavras(Origem)          # Cria a lista Palavras\r\n",
    "    Palavras, Eliminar = doRemoveChar(Palavras) # Cria a lista Eliminar\r\n",
    "    Palavras, Eliminar = doRemoveNumeros(Palavras, Eliminar)\r\n",
    "    return Origem, Palavras, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def doTrataArquivos(Palavras, Eliminar, Caminho):\r\n",
    "    ## TRATAMENTO DE ARQUIVOS ##\r\n",
    "    # Gera arquivos\r\n",
    "    if (doMapOcorrencia(Palavras, \"ML\", Caminho)):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_ML_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"KG\", Caminho)):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_KG_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"GR\", Caminho)):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_GR_verificado.csv\", Palavras )\r\n",
    "    #if (doMapOcorrencia(Palavras, \"GRAMAS\", Caminho)):\r\n",
    "    #    Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_GRAMAS_verificado.csv\", Palavras )\r\n",
    "    #if (doMapOcorrencia(Palavras, \"GRAMA\", Caminho)):\r\n",
    "    #    Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_GRAMA_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"G\", Caminho)):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_G_verificado.csv\", Palavras )\r\n",
    "    if (doMapQtd(Palavras, Caminho)):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_QUANTIDADE_verificado.csv\", Palavras )\r\n",
    "\r\n",
    "    return Palavras, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def verificaDescricao(Texto, Dicionario):\r\n",
    "    resp = ''\r\n",
    "   # palavraPermitida = df_lista_palavras['Palavra']\r\n",
    "   # dicPermitido = dict(zip(palavraPermitida, palavraPermitida))\r\n",
    "\r\n",
    "    frase = Texto.split(\" \")\r\n",
    "    resp = \"\"\r\n",
    "    for palavra in frase:\r\n",
    "        resp = resp + \" \" + Dicionario.get(palavra,'')\r\n",
    "    resp = resp.strip()\r\n",
    "    resp = resp.replace(\"  \",\" \")\r\n",
    "    return resp\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "def doGravaPalavras(Origem, Caminho):\r\n",
    "    #Salva as palavras da base, já com as devidas retiradas de palavras\r\n",
    "    Origem.to_csv(Caminho + \"ETL_base_lista_palavras_corretas.csv\", index = True, columns = [\"Palavra\", \"QTD\"], sep = ';', encoding = \"utf-8\")\r\n",
    "    Origem.to_pickle(Caminho + \"ETL_base_lista_palavras_corretas.pkl\")\r\n",
    "\r\n",
    "def doGrava(Origem, Caminho):\r\n",
    "    Origem.to_pickle(Caminho + \"ETL_base_pronta_para_previsao.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def doTratamento(Original, Caminho = \"..\\\\dados\\\\\"):\r\n",
    "    import import_ipynb\r\n",
    "    nltk.download('stopwords')\r\n",
    "    stop_words = set(stopwords.words('portuguese') )\r\n",
    "    \r\n",
    "    Original = doTrataColunas(Original, stop_words)\r\n",
    "    Eliminar = []  \r\n",
    "        \r\n",
    "    Original, Palavras, Eliminar = doTrataTextos(Original)\r\n",
    "    \r\n",
    "    Palavras, Eliminar = doTrataArquivos(Palavras, Eliminar, Caminho)\r\n",
    "    \r\n",
    "    doGravaPalavras(Palavras, Caminho)\r\n",
    "    \r\n",
    "    Dicionario = dict(zip(Palavras['Palavra'], Palavras['Palavra']))\r\n",
    "    Original['NovaDescricao'] = Original.apply(lambda row: verificaDescricao(row.NovaDescricao, Dicionario) , axis=1)\r\n",
    "    \r\n",
    "    doGrava(Palavras, Caminho)\r\n",
    "    return Original"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# SANDBOX\r\n",
    "import nltk\r\n",
    "import import_ipynb\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from carga import doCarga\r\n",
    "diretorio = '..\\\\dados\\\\'\r\n",
    "Original = doCarga(diretorio, opcao=4)\r\n",
    "Caminho = \"..\\\\dados\\\\\"\r\n",
    "\r\n",
    "\r\n",
    "nltk.download('stopwords')\r\n",
    "stop_words = set(stopwords.words('portuguese') )\r\n",
    "\r\n",
    "Original = doTrataColunas(Original, stop_words)\r\n",
    "Eliminar = []  \r\n",
    "\r\n",
    "Original, Palavras, Eliminar = doTrataTextos(Original)\r\n",
    "\r\n",
    "Palavras, Eliminar = doTrataArquivos(Palavras, Eliminar, Caminho)\r\n",
    "\r\n",
    "doGravaPalavras(Palavras, Caminho)\r\n",
    "\r\n",
    "Dicionario = dict(zip(Palavras['Palavra'], Palavras['Palavra']))\r\n",
    "Original['NovaDescricao'] = Original.apply(lambda row: verificaDescricao(row.NovaDescricao, Dicionario) , axis=1)\r\n",
    "\r\n",
    "doGrava(Original, Caminho)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\svpon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "Original['Categoria'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "31.0simsim    56845\n",
       "38.0simsim    22598\n",
       "38.0simnão    16433\n",
       "30.0simsim     3090\n",
       "31.0nãonão      180\n",
       "Name: Categoria, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "Original.describe"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of             Remetente   Categoria                      NovaDescricao\n",
       "0      Fornecedor 001  30.0simsim      CACHACA SAO FRANCISCO 12X970M\n",
       "1      Fornecedor 001  30.0simsim            CACHACA YPIOCA EMP.OURO\n",
       "2      Fornecedor 001  30.0simsim                CACHACA YPIOCA OURO\n",
       "3      Fornecedor 001  30.0simsim               CACHACA YPIOCA PRATA\n",
       "4      Fornecedor 002  30.0simsim  BB CACHACA SAGATIBA PURA  (12) GF\n",
       "...               ...         ...                                ...\n",
       "99995  Fornecedor 047  38.0simnão      SHAMP.INF.BEBE NATUREZA SUAVE\n",
       "99996  Fornecedor 047  38.0simnão      SHAMP.INF.BEBE NATUREZA SUAVE\n",
       "99997  Fornecedor 047  38.0simnão        SHAMP.INF.CARROS II MCQUEEN\n",
       "99998  Fornecedor 047  38.0simnão      SHAMP.INF.CARROS II MCQUEENNC\n",
       "99999  Fornecedor 047  38.0simnão      SHAMP.INF.CARROS II MCQUEENNC\n",
       "\n",
       "[99146 rows x 3 columns]>"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "print(Dicionario.get('AGUARDENTE'))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AGUARDENTE\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "Palavras['Palavra']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1          AGUARDENTE\n",
       "3                  GF\n",
       "4                  AG\n",
       "5              YPIOCA\n",
       "7              AGUARD\n",
       "            ...      \n",
       "551          (BRANCA)\n",
       "553               FAZ\n",
       "554            GRATIS\n",
       "555    ENVELHEC.750ML\n",
       "556              FRAN\n",
       "Name: Palavra, Length: 463, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# RECORTA FONTE\r\n",
    "\r\n",
    "import import_ipynb\r\n",
    "import numpy as np\r\n",
    "from carga import doCarga\r\n",
    "\r\n",
    "diretorio = '..\\\\dados\\\\'\r\n",
    "dfNFiscais = doCarga(diretorio, opcao=2) \r\n",
    "\r\n",
    "dfmini = dfNFiscais.iloc[0:100000]\r\n",
    "dfmini.describe\r\n",
    "dfmini.to_pickle(diretorio + \"df_Mini.pkl\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}