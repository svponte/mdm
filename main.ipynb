{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mineração de Dados Massivos\r\n",
    "## Analise de descrição de mercadorias de Notas Fiscais Eletrônicas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ambiente"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bibliotecas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Iniciando Ambiente\r\n",
    "import numpy as np\r\n",
    "import datetime as dt\r\n",
    "import re \r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import nltk\r\n",
    "import unicodedata\r\n",
    "\r\n",
    "# Se necessário, descomente as linhas a seguir para ler as stopwords\r\n",
    "nltk.download('stopwords')\r\n",
    "\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\svpon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Módulos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Módulos de código de programação\r\n",
    "#!pip install import-ipynb\r\n",
    "import import_ipynb\r\n",
    "from carga import *\r\n",
    "from tratamento import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variáveis de Ambiente"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Pasta aonde estão os dados\r\n",
    "diretorio = '..\\\\dados\\\\'\r\n",
    "#diretorio = \"F:\\\\Weisner\\\\Documentos\\\\MEGA Estudos\\\\UNB - MDM - Mineração de Dados Massivos\\\\Artigo\\\\\" \r\n",
    "arquivo   = \"Base_1milhão.xlsx\"\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Abre arquivo de dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Carrega os dados\r\n",
    "df_original = doCarga(diretorio, opcao=1)\r\n",
    "print (\"{} registros carregados\".format(df_original.size))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10153810 registros carregados\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "## SANDBOX\r\n",
    "df_original = doTratamento(df_original)\r\n",
    "df_original.NovaDescricao\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0          0 CACHACA SAO FRANCISCO 12X970M\\n1 CACHACA YPI...\n",
       "1          0 CACHACA SAO FRANCISCO 12X970M\\n1 CACHACA YPI...\n",
       "2          0 CACHACA SAO FRANCISCO 12X970M\\n1 CACHACA YPI...\n",
       "3          0 CACHACA SAO FRANCISCO 12X970M\\n1 CACHACA YPI...\n",
       "4          0 CACHACA SAO FRANCISCO 12X970M\\n1 CACHACA YPI...\n",
       "                                 ...                        \n",
       "1015376    0 CACHACA SAO FRANCISCO 12X970M\\n1 CACHACA YPI...\n",
       "1015377    0 CACHACA SAO FRANCISCO 12X970M\\n1 CACHACA YPI...\n",
       "1015378    0 CACHACA SAO FRANCISCO 12X970M\\n1 CACHACA YPI...\n",
       "1015379    0 CACHACA SAO FRANCISCO 12X970M\\n1 CACHACA YPI...\n",
       "1015380    0 CACHACA SAO FRANCISCO 12X970M\\n1 CACHACA YPI...\n",
       "Name: NovaDescricao, Length: 1015381, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepração dos Dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "df_original['Categoria'].value_counts().plot.bar()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "##from sklearn.datasets import fetch_20newsgroups\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn import metrics\r\n",
    "\r\n",
    "import sklearn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Função para avaliação dos modelos\r\n",
    "\r\n",
    "def avalia_modelo(clf, X, y):\r\n",
    "    resultados = sklearn.model_selection.cross_val_predict(clf, X, y, cv=5)\r\n",
    "    # print( pd.crosstab(y, resultados, rownames=['Real'], colnames=['Predito'], margins=True))\r\n",
    "    return np.mean(sklearn.model_selection.cross_val_score(clf, X, y, cv=5))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Obtem lista de fornecedores com mais de 10000 itens\r\n",
    "fornecedores = df['Remetente'].value_counts()\r\n",
    "mask = fornecedores > 10000\r\n",
    "fornecedores = list(fornecedores[mask].index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len( fornecedores )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_resultado     = [ [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan ] ]\r\n",
    "for i in range( len( fornecedores ) ):\r\n",
    "    df_resultado.append( [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan ] )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fornecedores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df_original.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cria o df de resultados\r\n",
    "colunas = [ 'Remetente','registros1','precisao1','registros2','precisao2','registros3','precisao3','registros4','precisao4' ]\r\n",
    "#df_resultado = [] #pd.DataFrame(columns = colunas)\r\n",
    "\r\n",
    "for fonteDados in range(0,2):\r\n",
    "    # fonteDados = 0 --> dados com as palavras separadas manualmente\r\n",
    "    # fonteDados = 1 --> dados sem as palavras separadas manualmente\r\n",
    "    \r\n",
    "    linha  = 0\r\n",
    "    coluna = 0\r\n",
    "\r\n",
    "    # retira as palavras separadas manualmente\r\n",
    "#    if fonteDados == 10:\r\n",
    "#        print( )\r\n",
    "#        print('Sem palavras selecionadas manualemnte.')\r\n",
    "#        print()\r\n",
    "#        \r\n",
    "#        # Lê o arquivo de paravras indesejadas \r\n",
    "#        df_palavras_a_eliminar = pd.read_csv('ETL_base_lista_palavras_a_retirar_minusculas.csv', sep=\";\")\r\n",
    "#        from nltk.corpus import stopwords\r\n",
    "#        stopwords = set( stopwords.words('portuguese') + list( df_palavras_a_eliminar['palavra'] ) )\r\n",
    "#\r\n",
    "#        # retira as palavras indesejadas do df\r\n",
    "#        novadescricao = map( limpaTexto2, df['NovaDescricao'] ) \r\n",
    "#        df.insert( loc = 2, column = \"NovaDescricao2\", value = list(novadescricao))#, allow_duplicates = False)\r\n",
    "#        del novadescricao \r\n",
    "#        del df['NovaDescricao']\r\n",
    "#        df.columns = ['Remetente', 'NovaDescricao', 'Categoria']  \r\n",
    "        \r\n",
    "#        linha  = 0\r\n",
    "#        coluna = 4\r\n",
    "        \r\n",
    "    # Cria um Pipeline - Classificador Composto\r\n",
    "    # vectorizer => transformer => classifier \r\n",
    "    text_clf = Pipeline([('counts' , CountVectorizer() ),\r\n",
    "                       #  ('tfidf', TfidfTransformer()),\r\n",
    "                         ('classifier'  , MultinomialNB()   ),\r\n",
    "                        ])\r\n",
    "    \r\n",
    "    # registra resultados\r\n",
    "    df_resultado[linha][          0] = 'Todos'\r\n",
    "    df_resultado[linha][ coluna + 1] = df.shape[0]\r\n",
    "    df_resultado[linha][ coluna + 2] = avalia_modelo(text_clf, df['NovaDescricao'], df['Categoria'])\r\n",
    "    \r\n",
    "    print(linha,' : ',coluna+1)\r\n",
    "    print( df_resultado[linha][          0] )\r\n",
    "    print( df_resultado[linha][ coluna + 1] )\r\n",
    "    print( df_resultado[linha][ coluna + 2] )\r\n",
    "    print()\r\n",
    "    \r\n",
    "    # Cria um Pipeline - Classificador Composto\r\n",
    "    # vectorizer => transformer => classifier \r\n",
    "    text_clf = Pipeline([('counts' , CountVectorizer() ),\r\n",
    "                         ('tfidf', TfidfTransformer()),\r\n",
    "                         ('classifier'  , MultinomialNB()   ),\r\n",
    "                        ])\r\n",
    "    \r\n",
    "    # registra resultados\r\n",
    "    df_resultado[linha][ coluna + 3] = df.shape[0]\r\n",
    "    df_resultado[linha][ coluna + 4] = avalia_modelo(text_clf, df['NovaDescricao'], df['Categoria'])    \r\n",
    "    \r\n",
    "    print(linha,' : ',coluna+3)\r\n",
    "    print( df_resultado[linha][          0] )\r\n",
    "    print( df_resultado[linha][ coluna + 3] )\r\n",
    "    print( df_resultado[linha][ coluna + 4] )\r\n",
    "    print()\r\n",
    "    \r\n",
    "    # calcula precisão para fornecedores\r\n",
    "    for fornecedor in fornecedores:\r\n",
    "        \r\n",
    "        linha += 1\r\n",
    "        \r\n",
    "        # obtem dados do fornecedor\r\n",
    "        mask = df[\"Remetente\"] == fornecedor\r\n",
    "        dados_fornecedor = df[ mask ]\r\n",
    "   \r\n",
    "\r\n",
    "        # Cria um Pipeline - Classificador Composto\r\n",
    "        # vectorizer => transformer => classifier \r\n",
    "        text_clf = Pipeline([('counts' , CountVectorizer() ),\r\n",
    "                           #  ('tfidf', TfidfTransformer()),\r\n",
    "                             ('classifier'  , MultinomialNB()   ),\r\n",
    "                            ])\r\n",
    "\r\n",
    "        # registra resultados\r\n",
    "        df_resultado[linha][          0] = fornecedor\r\n",
    "        df_resultado[linha][ coluna + 1] = dados_fornecedor.shape[0]\r\n",
    "        df_resultado[linha][ coluna + 2] = avalia_modelo(text_clf, dados_fornecedor['NovaDescricao'], \r\n",
    "                                                                   dados_fornecedor['Categoria'])\r\n",
    "        print(linha,' : ',coluna+1)\r\n",
    "        print( df_resultado[linha][          0] )\r\n",
    "        print( df_resultado[linha][ coluna + 1] )\r\n",
    "        print( df_resultado[linha][ coluna + 2] )\r\n",
    "        print()\r\n",
    "    \r\n",
    "        # Cria um Pipeline - Classificador Composto\r\n",
    "        # vectorizer => transformer => classifier \r\n",
    "        text_clf = Pipeline([('counts' , CountVectorizer() ),\r\n",
    "                             ('tfidf', TfidfTransformer()),\r\n",
    "                             ('classifier'  , MultinomialNB()   ),\r\n",
    "                            ])\r\n",
    "\r\n",
    "        # registra resultados\r\n",
    "        df_resultado[linha][ coluna + 3] = dados_fornecedor.shape[0]\r\n",
    "        df_resultado[linha][ coluna + 4] = avalia_modelo(text_clf, dados_fornecedor['NovaDescricao'], \r\n",
    "                                                                   dados_fornecedor['Categoria'])         \r\n",
    "        print(linha,' : ',coluna+3)\r\n",
    "        print( df_resultado[linha][          0] )\r\n",
    "        print( df_resultado[linha][ coluna + 3] )\r\n",
    "        print( df_resultado[linha][ coluna + 4] )\r\n",
    "        print()\r\n",
    "    \r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_resultado"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create the transform\r\n",
    "vectorizer = CountVectorizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = df['NovaDescricao']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tokenize and build vocab\r\n",
    "vectorizer.fit(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# summarize\r\n",
    "print(vectorizer.vocabulary_)\r\n",
    "# encode document\r\n",
    "vector = vectorizer.transform(text)\r\n",
    "# summarize encoded vector\r\n",
    "print(vector.shape)\r\n",
    "print(type(vector))\r\n",
    "print(vector.toarray())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(vectorizer.vocabulary_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(vectorizer.vocabulary_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "palavras = list(vectorizer.vocabulary_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "palavras.sort()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(palavras)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "palavras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_palavras = pd.DataFrame(data = palavras)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_palavras.to_csv('palavras.cvs', sep=\";\", index=False )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pwd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocabulário = list( vectorizer.vocabulary_ )\r\n",
    "vocabulário"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vocabulário.sort()\r\n",
    "vocabulário"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mineração"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}