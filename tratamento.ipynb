{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tramento dos dados brutos\r\n",
    "\r\n",
    "Após a carga dos dataset a partir dos arquivos (doCarga), nesta etapa os dados sofrerão limpesa e tratamento, aonde serão retiradas \"sugeiras grosseiras\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import unicodedata\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tratamentos de Colunas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# retira stop words do nome dos usuários reais\r\n",
    "def limpaTexto( entrada ):\r\n",
    "    \r\n",
    "    listaRetiraInicio = [] # ['BB','Z ', 'MC ']\r\n",
    "    listaRetiraFim    = [] # [' CC']\r\n",
    "    listaCaracteresIndesejados = [\"\t\", # tabulação\r\n",
    "                                \"*\",\r\n",
    "                                \"#\",\r\n",
    "                                \"|\",\r\n",
    "                                '¥'\r\n",
    "                                ]\r\n",
    "\r\n",
    "    # PROBLEMA: TEM QUE VERIFICAR A LISTA DE STOPWORDS PARA QUE ELA \r\n",
    "    # NÃO RETIRE DESCRIÇÃO VÁLIDA DE PRODUTOS.\r\n",
    "    # retira stopwords\r\n",
    "    entrada = str(entrada)\r\n",
    "    entrada = entrada.split(\" \")\r\n",
    "    saida = \"\"\r\n",
    "    for item in entrada:\r\n",
    "        if item.lower() not in lstStopWords:\r\n",
    "            saida = saida + item + \" \"\r\n",
    "    \r\n",
    "    # retira acentos\r\n",
    "    aux   = str(saida)\r\n",
    "    nfkd  = unicodedata.normalize('NFKD', aux)\r\n",
    "    saida = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\r\n",
    "    \r\n",
    "    # transforma em maiúsculo, retira espaços dos extremos (TRIM), \r\n",
    "    saida = saida.upper().strip()\r\n",
    "    \r\n",
    "    # retira caracteres indesejados --> cuidado para não unir palavras separadas pelo caracter\r\n",
    "    for item in listaCaracteresIndesejados:\r\n",
    "        saida = saida.replace(item,\"\")    \r\n",
    "    \r\n",
    "    # retira espaços duplos dentro da string\r\n",
    "    saida = saida.replace(\"  \",\" \")\r\n",
    "    while saida.find('  ') != -1:\r\n",
    "        saida = saida.replace(\"  \",\" \")\r\n",
    "              \r\n",
    "    # retira inicios de strings que constam na lista listaRetiraInicio\r\n",
    "    for item in listaRetiraInicio:\r\n",
    "        # retira espaços duplos dentro do item, pois o mesmo já foi feito no texto a pesquisar\r\n",
    "        item = item.replace(\"  \",\" \")\r\n",
    "        while item.find('  ') != -1:\r\n",
    "            item = item.replace(\"  \",\" \")\r\n",
    "            \r\n",
    "        if saida.startswith(item):\r\n",
    "            tamanhoItem  = len(item)\r\n",
    "            tamanhoSaida = len(saida)            \r\n",
    "            if tamanhoSaida > tamanhoItem:\r\n",
    "                saida = saida[tamanhoItem:]\r\n",
    "    \r\n",
    "    # retira fins de strings que constam na lista listaRetiraFim\r\n",
    "    for item in listaRetiraFim:\r\n",
    "        # retira espaços duplos dentro do item, pois o mesmo já foi feito no texto a pesquisar\r\n",
    "        item = item.replace(\"  \",\" \")\r\n",
    "        while item.find('  ') != -1:\r\n",
    "            item = item.replace(\"  \",\" \")        \r\n",
    "        \r\n",
    "        if saida.endswith(item):\r\n",
    "            tamanhoItem  = len(item)\r\n",
    "            tamanhoSaida = len(saida)\r\n",
    "            if tamanhoSaida > tamanhoItem:\r\n",
    "                saida = saida[:tamanhoSaida-tamanhoItem]   \r\n",
    "\r\n",
    "    # retira espaços duplos dentro da string. Necessário pois pode ter sobrado \r\n",
    "    # espaço depois das alterações\r\n",
    "    saida = saida.strip()\r\n",
    "        \r\n",
    "    return saida "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def doLimpeza(df_original):\r\n",
    "    # Apaga linhas cuja coluna 'NovaDescricao' esteja em branco\r\n",
    "    df_original['NovaDescricao'].dropna( axis=0, inplace=True ) \r\n",
    "\r\n",
    "    # Apaga colunas que não serão usadas\r\n",
    "    df_original.drop(['Desc_Catalogo', 'NCM_NFe', 'Desc_Anexo_IV', 'NCM_Calc_Prov', 'Cor_NCM_Calc', 'NCM_Calc', \r\n",
    "         'Item_Anexo_IV', 'Prod_ST', 'Prod_FCP'], axis=1, inplace=True)\r\n",
    "    \r\n",
    "    return df_original"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def doDropDuplicados(df_original):\r\n",
    "    ## Elimina linhas inconsistentes, repetidas. Mantém a primeira delas\r\n",
    "    antes = df_original.shape[0]\r\n",
    "    df_original.drop_duplicates( subset = [\"Categoria\"], inplace = True) \r\n",
    "    print( 'Eliminados', antes - df_original.shape[0], 'registros')\r\n",
    "    del antes\r\n",
    "    return df_original "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def doDropIndesejados(df_original):\r\n",
    "    # Retira categorias indesejadas\r\n",
    "    retirar = ['nannãonão','nansimnão','nansimsim']\r\n",
    "    mask = ~df_original['Categoria'].isin(retirar)\r\n",
    "    filtrado = df_original[mask].copy()\r\n",
    "    df_original = filtrado.copy()\r\n",
    "    del filtrado, retirar\r\n",
    "    return df_original"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tratamento de Strings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def doListaPalavras(df):\r\n",
    "    # Gera lista com todas as palavras, com repetições\r\n",
    "    # CountVectorizer retira duplicidade\r\n",
    "    total_linhas = df.shape[0]\r\n",
    "    lista_palavras = []\r\n",
    "    for i in range(0, total_linhas):\r\n",
    "        texto = df.iloc[i, 2].split()\r\n",
    "        tamanho_linha = len(texto)\r\n",
    "        for j in range(0,tamanho_linha):\r\n",
    "            lista_palavras.append(texto[j]) \r\n",
    "            \r\n",
    "    # Gera DF com a contagem de cada palavra\r\n",
    "    lista_palavras = pd.Series(data = lista_palavras)\r\n",
    "    df_lista_palavras = pd.DataFrame(data = lista_palavras.value_counts() )#, index = lista_palavras)\r\n",
    "    df_lista_palavras.reset_index(drop = False, inplace = True)\r\n",
    "    df_lista_palavras.columns = ['Palavra', 'QTD']\r\n",
    "\r\n",
    "    #libera memoria\r\n",
    "    del total_linhas, lista_palavras, texto, tamanho_linha,\r\n",
    "    return df_lista_palavras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def doRemoveChar(Lista):\r\n",
    "    # elimina palavras de 1 caracter\r\n",
    "    mask = (Lista['Palavra'].str.len() == 1)\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Eliminar = []\r\n",
    "    Eliminar = list(df_aux['Palavra'])\r\n",
    "    mask = ~(Lista['Palavra'].str.len() == 1)\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Lista = df_aux.copy()\r\n",
    "\r\n",
    "    del df_aux, mask\r\n",
    "    return Lista, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def doRemoveNumeros(Lista, Eliminar):\r\n",
    "    # Elimina palavras que são somente numeros, exceto os de 13 algarismos ( possíveis EAN)\r\n",
    "\r\n",
    "    # Separa possíveis códigos EAN\r\n",
    "    mask1 = Lista.Palavra.str.len() == 13\r\n",
    "    mask2 = Lista.Palavra.str.isdigit()\r\n",
    "    df_aux = Lista[~(mask1 & mask2)]\r\n",
    "    # elimina números\r\n",
    "    mask2 = df_aux.Palavra.str.isdigit()\r\n",
    "    df_aux = df_aux[mask2]\r\n",
    "\r\n",
    "    Eliminar = Eliminar + list(df_aux['Palavra'])\r\n",
    "    mask = ~Lista['Palavra'].isin( list(df_aux['Palavra']) )\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Lista = df_aux.copy()\r\n",
    "    del df_aux, mask1, mask2\r\n",
    "    return Lista, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tratamento de Arquivos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def doMapOcorrencia(Origem, termo):\r\n",
    "    result = False\r\n",
    "    mask1  = Origem['Palavra'].str.endswith(termo)\r\n",
    "    mask2  = Origem['Palavra'].str.len() < 80\r\n",
    "    mask3  = Origem['QTD'] > 10 \r\n",
    "    df_aux = Origem[ mask1 & mask2 & mask3 ]\r\n",
    "\r\n",
    "    if df_aux.shape[0] > 1:\r\n",
    "        df_aux.to_csv(\"ETL_\" + termo + \"_a_verificar.csv\", index = True, columns = [\"Palavra\", \"QTD\"], sep = ';', encoding = \"utf-8\")\r\n",
    "        result = True\r\n",
    "    else:\r\n",
    "        print( 'Não há palavras terminadas com \"{}\", ou há somente uma.'.format(termo) )\r\n",
    "    del df_aux, mask1, mask2, mask3\r\n",
    "    return result \r\n",
    "\r\n",
    "def doMapQtd(Origem):\r\n",
    "    #Trata palavras pela quantidade delas\r\n",
    "    result = False\r\n",
    "    mask  = Origem['QTD'] > 1000 \r\n",
    "    df_aux = Origem[ mask ]\r\n",
    "    if df_aux.shape[0] > 1:\r\n",
    "        df_aux.to_csv(\"ETL_QUANTIDADE_a_verificar.csv\", index = True, columns = [\"Palavra\", \"QTD\"], sep = ';', encoding = \"utf-8\")\r\n",
    "        result = True\r\n",
    "    else:\r\n",
    "        print( 'Não há palavras mais de 1000 ocorrências' )\r\n",
    "    del df_aux, mask\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def EliminaPalavrasVindasDeArquivo( nome_arquivo, Lista ):\r\n",
    "    df_aux = pd.read_csv(nome_arquivo, index_col = ['Indice'], sep = ';',)\r\n",
    "    Lista.append(list(df_aux['Palavra']))\r\n",
    "    mask = ~Lista['Palavra'].isin( list(df_aux['Palavra']) )\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Lista = df_aux.copy()\r\n",
    "    del df_aux, mask\r\n",
    "    return Lista"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execução"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def doTrataColunas(Origem):\r\n",
    "    ## TRATAMENTO DE COLUNAS ##\r\n",
    "\r\n",
    "    # Concatena 3 colunas para formar a coluna 'Categoria', que é a classificação do produto\r\n",
    "    Origem['Categoria'] = Origem['Item_Anexo_IV'].astype(str) + Origem['Prod_ST'] + Origem['Prod_FCP']\r\n",
    "    Origem[\"NovaDescricao\"] = limpaTexto(Origem['Desc_Catalogo'])\r\n",
    "    Origem = doLimpeza(Origem)    \r\n",
    "\r\n",
    "    # Renomeia as colunas do DF\r\n",
    "    #df_original.columns = ('Remetente', 'NovaDescricao', 'Categoria') ## <<< ACHO Q ESTA INVERTIDO !!!!!!!!!!!!!!\r\n",
    "    Origem.columns = ('Remetente', 'Categoria', 'NovaDescricao') \r\n",
    "\r\n",
    "    Origem = doDropDuplicados(Origem)\r\n",
    "    Origem = doDropIndesejados(Origem)\r\n",
    "    return Origem"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def doTrataTextos(Origem):\r\n",
    "    ## TRATAMENTO DE TEXTOS ##\r\n",
    "    Palavras = doListaPalavras(Origem)          # Cria a lista Palavras\r\n",
    "    Palavras, Eliminar = doRemoveChar(Palavras) # Cria a lista Eliminar\r\n",
    "    Palavras, Eliminar = doRemoveNumeros(Palavras, Eliminar)\r\n",
    "    return Origem, Palavras, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def doTrataArquivos(Palavras, Eliminar, Caminho):\r\n",
    "    ## TRATAMENTO DE ARQUIVOS ##\r\n",
    "    # Gera arquivos\r\n",
    "    if (doMapOcorrencia(Palavras, \"ML\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_ML_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"KG\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_KG_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"GR\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_GR_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"GRAMAS\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_GRAMAS_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"GRAMA\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_GRAMA_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"G\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_G_verificado.csv\", Palavras )\r\n",
    "    if (doMapQtd(Palavras)):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_QUANTIDADE_verificado.csv\", Palavras )\r\n",
    "\r\n",
    "    return Palavras, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def verificaDescricao(frase, Dicionario):\r\n",
    "    resp = ''\r\n",
    "   # palavraPermitida = df_lista_palavras['Palavra']\r\n",
    "   # dicPermitido = dict(zip(palavraPermitida, palavraPermitida))\r\n",
    "\r\n",
    "    frase = frase.split(\" \")\r\n",
    "    for palavra in frase:\r\n",
    "        resp = resp + \" \" + Dicionario.get(palavra,'')\r\n",
    "    resp = resp.strip()\r\n",
    "    resp = resp.replace(\"  \",\" \")\r\n",
    "    return resp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def doGravaPalavras(Origem, Caminho):\r\n",
    "    #Salva as palavras da base, já com as devidas retiradas de palavras\r\n",
    "    Origem.to_csv(Caminho + \"ETL_base_lista_palavras_corretas.csv\", index = True, columns = [\"Palavra\", \"QTD\"], sep = ';', encoding = \"utf-8\")\r\n",
    "    Origem.to_pickle(Caminho + \"ETL_base_lista_palavras_corretas.pkl\")\r\n",
    "\r\n",
    "def doGrava(Origem, Caminho):\r\n",
    "    Origem.to_pickle(Caminho + \"ETL_base_pronta_para_previsao.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def doTratamento(Original, Caminho = \"..\\\\dados\\\\\"):\r\n",
    "    lstStopWords = set(stopwords.words('portuguese') )\r\n",
    "    \r\n",
    "    Original = doTrataColunas(Original)\r\n",
    "    Eliminar = []    \r\n",
    "    Original, Palavras, Eliminar = doTrataTextos(Original)\r\n",
    "    \r\n",
    "    Palavras, Eliminar = doTrataArquivos(Palavras, Eliminar, Caminho)\r\n",
    "    \r\n",
    "    doGravaPalavras(Palavras, Caminho)\r\n",
    "    \r\n",
    "    #aux = list( map(lambda x: verificaDescricao(x, Palavras), list(Original['NovaDescricao']) ) ) \r\n",
    "    #Original['NovaDescricao'] = aux\r\n",
    "    \r\n",
    "    print ('kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk')\r\n",
    "    print (Palavras)\r\n",
    "    print ('dddddddddddddddddddddddddddddddddddddddddddddddddddddd')\r\n",
    "\r\n",
    "    doGrava(Palavras, Caminho)\r\n",
    "    return Original"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# SANDBOX\r\n",
    "\r\n",
    "import import_ipynb\r\n",
    "from carga import *\r\n",
    "nltk.download('stopwords')\r\n",
    "lstStopWords = set(stopwords.words('portuguese'))\r\n",
    "diretorio = '..\\\\dados\\\\'\r\n",
    "df_original = doCarga(diretorio, opcao=1)  # 2 df_originalAnonimo\r\n",
    "\r\n",
    "doTratamento(df_original)\r\n",
    "\r\n",
    "#df_original"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from carga.ipynb\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\svpon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "<string>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<string>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<string>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}