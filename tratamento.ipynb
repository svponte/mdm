{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tramento dos dados brutos\r\n",
    "\r\n",
    "Após a carga dos dataset a partir dos arquivos (doCarga), nesta etapa os dados sofrerão limpesa e tratamento, aonde serão retiradas \"sugeiras grosseiras\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import unicodedata\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tratamentos de Colunas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "source": [
    "# retira stop words do nome dos usuários reais\r\n",
    "def limpaTexto( entrada, stop_words ):\r\n",
    "    \r\n",
    "    listaRetiraInicio = [] # ['BB','Z ', 'MC ']\r\n",
    "    listaRetiraFim    = [] # [' CC']\r\n",
    "    listaCaracteresIndesejados = [\"\t\", # tabulação\r\n",
    "                                \"*\",\r\n",
    "                                \"#\",\r\n",
    "                                \"|\",\r\n",
    "                                '¥'\r\n",
    "                                ]\r\n",
    "\r\n",
    "    # PROBLEMA: TEM QUE VERIFICAR A LISTA DE STOPWORDS PARA QUE ELA \r\n",
    "    # NÃO RETIRE DESCRIÇÃO VÁLIDA DE PRODUTOS.\r\n",
    "    # retira stopwords\r\n",
    "    entrada = str(entrada)\r\n",
    "    entrada = entrada.split(\" \")\r\n",
    "    \r\n",
    "    saida = \"\"\r\n",
    "    for item in entrada:\r\n",
    "        if item.lower() not in stop_words :\r\n",
    "            saida = saida + item + \" \"\r\n",
    "    \r\n",
    "    # retira acentos\r\n",
    "    aux   = str(saida)\r\n",
    "    nfkd  = unicodedata.normalize('NFKD', aux)\r\n",
    "    saida = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\r\n",
    "    \r\n",
    "    # transforma em maiúsculo, retira espaços dos extremos (TRIM), \r\n",
    "    saida = saida.upper().strip()\r\n",
    "    \r\n",
    "    # retira caracteres indesejados --> cuidado para não unir palavras separadas pelo caracter\r\n",
    "    for item in listaCaracteresIndesejados:\r\n",
    "        saida = saida.replace(item,\"\")    \r\n",
    "    \r\n",
    "    # retira espaços duplos dentro da string\r\n",
    "    saida = saida.replace(\"  \",\" \")\r\n",
    "    while saida.find('  ') != -1:\r\n",
    "        saida = saida.replace(\"  \",\" \")\r\n",
    "              \r\n",
    "    # retira inicios de strings que constam na lista listaRetiraInicio\r\n",
    "    for item in listaRetiraInicio:\r\n",
    "        # retira espaços duplos dentro do item, pois o mesmo já foi feito no texto a pesquisar\r\n",
    "        item = item.replace(\"  \",\" \")\r\n",
    "        while item.find('  ') != -1:\r\n",
    "            item = item.replace(\"  \",\" \")\r\n",
    "            \r\n",
    "        if saida.startswith(item):\r\n",
    "            tamanhoItem  = len(item)\r\n",
    "            tamanhoSaida = len(saida)            \r\n",
    "            if tamanhoSaida > tamanhoItem:\r\n",
    "                saida = saida[tamanhoItem:]\r\n",
    "    \r\n",
    "    # retira fins de strings que constam na lista listaRetiraFim\r\n",
    "    for item in listaRetiraFim:\r\n",
    "        # retira espaços duplos dentro do item, pois o mesmo já foi feito no texto a pesquisar\r\n",
    "        item = item.replace(\"  \",\" \")\r\n",
    "        while item.find('  ') != -1:\r\n",
    "            item = item.replace(\"  \",\" \")        \r\n",
    "        \r\n",
    "        if saida.endswith(item):\r\n",
    "            tamanhoItem  = len(item)\r\n",
    "            tamanhoSaida = len(saida)\r\n",
    "            if tamanhoSaida > tamanhoItem:\r\n",
    "                saida = saida[:tamanhoSaida-tamanhoItem]   \r\n",
    "    \r\n",
    "    # retira espaços duplos dentro da string. Necessário pois pode ter sobrado \r\n",
    "    # espaço depois das alterações\r\n",
    "    saida = saida.strip()\r\n",
    "    #print (saida) ####################################################################################################\r\n",
    "    #saida = list(saida.split(\"\\n\"))\r\n",
    "    #print (type(saida))    \r\n",
    "    return saida"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "source": [
    "def doLimpeza(df_original):\r\n",
    "    # Apaga linhas cuja coluna 'NovaDescricao' esteja em branco\r\n",
    "    df_original['NovaDescricao'].dropna( axis=0, inplace=True ) \r\n",
    "\r\n",
    "\r\n",
    "     # Apaga colunas que não serão usadas\r\n",
    "    df_original.drop(['Desc_Catalogo', 'NCM_NFe', 'Desc_Anexo_IV', 'NCM_Calc_Prov', 'Cor_NCM_Calc', 'NCM_Calc', \r\n",
    "        'Item_Anexo_IV', 'Prod_ST', 'Prod_FCP'], axis=1, inplace=True)\r\n",
    "    \r\n",
    "    return df_original"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "source": [
    "def doDropDuplicados(df_original):\r\n",
    "    ## Elimina linhas inconsistentes, repetidas. Mantém a primeira delas\r\n",
    "    #antes = df_original.shape[0]\r\n",
    "    df_original.drop_duplicates( subset = [\"NovaDescricao\"], keep='first', inplace=True) \r\n",
    "    #print( 'Eliminados', antes - df_original.shape[0], 'registros')\r\n",
    "    #del antes\r\n",
    "    return df_original "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "source": [
    "def doDropIndesejados(df_original):\r\n",
    "    # Retira categorias indesejadas\r\n",
    "    retirar = ['nannãonão','nansimnão','nansimsim']\r\n",
    "    mask = ~df_original['Categoria'].isin(retirar)\r\n",
    "    filtrado = df_original[mask].copy()\r\n",
    "    df_original = filtrado.copy()\r\n",
    "    del filtrado, retirar\r\n",
    "    return df_original"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tratamento de Strings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "source": [
    "def doListaPalavras(df):\r\n",
    "    # Gera lista com todas as palavras, com repetições\r\n",
    "    # CountVectorizer retira duplicidade\r\n",
    "    \r\n",
    "    \r\n",
    "    lista_palavras = []\r\n",
    "    for item in df['NovaDescricao']:\r\n",
    "        items = str(item).split(' ')\r\n",
    "        for palavra in items:\r\n",
    "            lista_palavras.append(palavra)\r\n",
    "\r\n",
    "\r\n",
    "  #  total_linhas = df.shape[0]\r\n",
    "  #  lista_palavras = []\r\n",
    "  #  for i in range(0,total_linhas):\r\n",
    "  #      texto = df.iloc[i, 2].split()\r\n",
    "  #      tamanho_linha = len(texto)\r\n",
    "  #      for j in range(0,tamanho_linha):\r\n",
    "  #          lista_palavras.append( texto[j]) \r\n",
    "\r\n",
    "\r\n",
    "    # Gera DF com a contagem de cada palavra\r\n",
    "    #lista_palavras = pd.Series(data = lista_palavras)\r\n",
    "    #df_lista_palavras = pd.DataFrame(data = lista_palavras.value_counts() )#, index = lista_palavras)\r\n",
    "    \r\n",
    "    dict = {i:lista_palavras.count(i) for i in lista_palavras}\r\n",
    "    df_lista_palavras = pd.DataFrame(dict)\r\n",
    "    df_lista_palavras.reset_index(drop = False, inplace = True)\r\n",
    "    df_lista_palavras.columns = ['Palavra', 'QTD']\r\n",
    "\r\n",
    "    #libera memoria\r\n",
    "    del lista_palavras #total_linhas, texto, tamanho_linha,\r\n",
    "    print (df_lista_palavras)\r\n",
    "    return df_lista_palavras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "source": [
    "def doRemoveChar(Lista):\r\n",
    "    # elimina palavras de 1 caracter\r\n",
    "    mask = (Lista['Palavra'].str.len() == 1)\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Eliminar = []\r\n",
    "    Eliminar = list(df_aux['Palavra'])\r\n",
    "    mask = ~(Lista['Palavra'].str.len() == 1)\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Lista = df_aux.copy()\r\n",
    "\r\n",
    "    del df_aux, mask\r\n",
    "    return Lista, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "source": [
    "def doRemoveNumeros(Lista, Eliminar):\r\n",
    "    # Elimina palavras que são somente numeros, exceto os de 13 algarismos ( possíveis EAN)\r\n",
    "\r\n",
    "    # Separa possíveis códigos EAN\r\n",
    "    mask1 = Lista.Palavra.str.len() == 13\r\n",
    "    mask2 = Lista.Palavra.str.isdigit()\r\n",
    "    df_aux = Lista[~(mask1 & mask2)]\r\n",
    "    # elimina números\r\n",
    "    mask2 = df_aux.Palavra.str.isdigit()\r\n",
    "    df_aux = df_aux[mask2]\r\n",
    "\r\n",
    "    Eliminar = Eliminar + list(df_aux['Palavra'])\r\n",
    "    mask = ~Lista['Palavra'].isin( list(df_aux['Palavra']) )\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Lista = df_aux.copy()\r\n",
    "    del df_aux, mask1, mask2\r\n",
    "    return Lista, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tratamento de Arquivos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "source": [
    "def doMapOcorrencia(Origem, termo):\r\n",
    "    result = False\r\n",
    "    mask1  = Origem['Palavra'].str.endswith(termo)\r\n",
    "    mask2  = Origem['Palavra'].str.len() < 80\r\n",
    "    mask3  = Origem['QTD'] > 10 \r\n",
    "    df_aux = Origem[ mask1 & mask2 & mask3 ]\r\n",
    "\r\n",
    "    if df_aux.shape[0] > 1:\r\n",
    "        df_aux.to_csv(\"ETL_\" + termo + \"_a_verificar.csv\", index = True, columns = [\"Palavra\", \"QTD\"], sep = ';', encoding = \"utf-8\")\r\n",
    "        result = True\r\n",
    "    #else:\r\n",
    "        #print( 'Não há palavras terminadas com \"{}\", ou há somente uma.'.format(termo) )\r\n",
    "    del df_aux, mask1, mask2, mask3\r\n",
    "    return result \r\n",
    "\r\n",
    "def doMapQtd(Origem):\r\n",
    "    #Trata palavras pela quantidade delas\r\n",
    "    result = False\r\n",
    "    mask  = Origem['QTD'] > 1000 \r\n",
    "    df_aux = Origem[ mask ]\r\n",
    "    if df_aux.shape[0] > 1:\r\n",
    "        df_aux.to_csv(\"ETL_QUANTIDADE_a_verificar.csv\", index = True, columns = [\"Palavra\", \"QTD\"], sep = ';', encoding = \"utf-8\")\r\n",
    "        result = True\r\n",
    "    #else:\r\n",
    "        #print( 'Não há palavras mais de 1000 ocorrências' )\r\n",
    "    del df_aux, mask\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "source": [
    "def EliminaPalavrasVindasDeArquivo( nome_arquivo, Lista ):\r\n",
    "    df_aux = pd.read_csv(nome_arquivo, index_col = ['Indice'], sep = ';',)\r\n",
    "    Lista.append(list(df_aux['Palavra']))\r\n",
    "    mask = ~Lista['Palavra'].isin( list(df_aux['Palavra']) )\r\n",
    "    df_aux = Lista[mask]\r\n",
    "    Lista = df_aux.copy()\r\n",
    "    del df_aux, mask\r\n",
    "    return Lista"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execução"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "source": [
    "def doTrataColunas(Origem, stop_words ):\r\n",
    "    ## TRATAMENTO DE COLUNAS ##\r\n",
    "    \r\n",
    "    # Concatena 3 colunas para formar a coluna 'Categoria', que é a classificação do produto\r\n",
    "    Origem['Categoria'] = Origem['Item_Anexo_IV'].astype(str) + Origem['Prod_ST'] + Origem['Prod_FCP']\r\n",
    "    #Origem[\"NovaDescricao\"] = limpaTexto(Origem['Desc_Catalogo'], stop_words )\r\n",
    "    \r\n",
    "    Origem[\"NovaDescricao\"] = map( limpaTexto, Origem['Desc_Catalogo'], stop_words )\r\n",
    "\r\n",
    "    Origem.rename(columns={'Nome': 'Remetente'}, inplace = True)\r\n",
    " \r\n",
    "    Origem = doLimpeza(Origem)   \r\n",
    "\r\n",
    "    Origem = doDropDuplicados(Origem)\r\n",
    "\r\n",
    "    return Origem"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "source": [
    "def doTrataTextos(Origem):\r\n",
    "    ## TRATAMENTO DE TEXTOS ##\r\n",
    "    Palavras = doListaPalavras(Origem)          # Cria a lista Palavras\r\n",
    "    Palavras, Eliminar = doRemoveChar(Palavras) # Cria a lista Eliminar\r\n",
    "    Palavras, Eliminar = doRemoveNumeros(Palavras, Eliminar)\r\n",
    "    return Origem, Palavras, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "source": [
    "def doTrataArquivos(Palavras, Eliminar, Caminho):\r\n",
    "    ## TRATAMENTO DE ARQUIVOS ##\r\n",
    "    # Gera arquivos\r\n",
    "    if (doMapOcorrencia(Palavras, \"ML\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_ML_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"KG\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_KG_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"GR\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_GR_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"GRAMAS\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_GRAMAS_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"GRAMA\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_GRAMA_verificado.csv\", Palavras )\r\n",
    "    if (doMapOcorrencia(Palavras, \"G\")):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_G_verificado.csv\", Palavras )\r\n",
    "    if (doMapQtd(Palavras)):\r\n",
    "        Palavras = EliminaPalavrasVindasDeArquivo( Caminho + \"ETL_QUANTIDADE_verificado.csv\", Palavras )\r\n",
    "\r\n",
    "    return Palavras, Eliminar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "source": [
    "def verificaDescricao(frase, Dicionario):\r\n",
    "    resp = ''\r\n",
    "   # palavraPermitida = df_lista_palavras['Palavra']\r\n",
    "   # dicPermitido = dict(zip(palavraPermitida, palavraPermitida))\r\n",
    "\r\n",
    "    frase = frase.split(\" \")\r\n",
    "    for palavra in frase:\r\n",
    "        resp = resp + \" \" + Dicionario.get(palavra,'')\r\n",
    "    resp = resp.strip()\r\n",
    "    resp = resp.replace(\"  \",\" \")\r\n",
    "    return resp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "source": [
    "def doGravaPalavras(Origem, Caminho):\r\n",
    "    #Salva as palavras da base, já com as devidas retiradas de palavras\r\n",
    "    Origem.to_csv(Caminho + \"ETL_base_lista_palavras_corretas.csv\", index = True, columns = [\"Palavra\", \"QTD\"], sep = ';', encoding = \"utf-8\")\r\n",
    "    Origem.to_pickle(Caminho + \"ETL_base_lista_palavras_corretas.pkl\")\r\n",
    "\r\n",
    "def doGrava(Origem, Caminho):\r\n",
    "    Origem.to_pickle(Caminho + \"ETL_base_pronta_para_previsao.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "source": [
    "def doTratamento(Original, Caminho = \"..\\\\dados\\\\\"):\r\n",
    "    import import_ipynb\r\n",
    "    nltk.download('stopwords')\r\n",
    "    stop_words = set(stopwords.words('portuguese') )\r\n",
    "    \r\n",
    "    Original = doTrataColunas(Original, stop_words)\r\n",
    "    Eliminar = []  \r\n",
    "        \r\n",
    "    Original, Palavras, Eliminar = doTrataTextos(Original)\r\n",
    "    \r\n",
    "    Palavras, Eliminar = doTrataArquivos(Palavras, Eliminar, Caminho)\r\n",
    "    \r\n",
    "    doGravaPalavras(Palavras, Caminho)\r\n",
    "    \r\n",
    "    #aux = list( map(lambda x: verificaDescricao(x, Palavras), list(Original['NovaDescricao']) ) ) \r\n",
    "    #Original['NovaDescricao'] = aux\r\n",
    "    \r\n",
    "    doGrava(Palavras, Caminho)\r\n",
    "    return Original"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "source": [
    "# SANDBOX\r\n",
    "\r\n",
    "import import_ipynb\r\n",
    "from carga import *\r\n",
    "nltk.download('stopwords')\r\n",
    "stop_words  = set(stopwords.words('portuguese'))\r\n",
    "diretorio = '..\\\\dados\\\\'\r\n",
    "df_original = doCarga(diretorio, opcao=2)  # 2 df_originalAnonimo\r\n",
    "\r\n",
    "doTratamento(df_original)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\svpon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\svpon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-612-0786c6a9f9f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdf_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoCarga\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiretorio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopcao\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 2 df_originalAnonimo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdoTratamento\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_original\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-611-2bbed0b04f21>\u001b[0m in \u001b[0;36mdoTratamento\u001b[1;34m(Original, Caminho)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mEliminar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mOriginal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPalavras\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEliminar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoTrataTextos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mPalavras\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEliminar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoTrataArquivos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPalavras\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEliminar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCaminho\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-607-09cf2250f241>\u001b[0m in \u001b[0;36mdoTrataTextos\u001b[1;34m(Origem)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdoTrataTextos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOrigem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m## TRATAMENTO DE TEXTOS ##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mPalavras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoListaPalavras\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOrigem\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# Cria a lista Palavras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mPalavras\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEliminar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoRemoveChar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPalavras\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Cria a lista Eliminar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mPalavras\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEliminar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoRemoveNumeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPalavras\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEliminar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-601-7c57795d8fd0>\u001b[0m in \u001b[0;36mdoListaPalavras\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#df_lista_palavras = pd.DataFrame(data = lista_palavras.value_counts() )#, index = lista_palavras)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mdf_lista_palavras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlista_palavras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlista_palavras\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdf_lista_palavras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         ]\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"If using all scalar values, you must pass an index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhave_series\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}